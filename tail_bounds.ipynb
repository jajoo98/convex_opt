{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be031f2a",
   "metadata": {},
   "source": [
    "## Question 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "82a44c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cvxpy as cp\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import multivariate_normal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "274732f1",
   "metadata": {},
   "source": [
    "### Proving tail bounds for log-concave densities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29131e47",
   "metadata": {},
   "source": [
    "Let $X \\in \\mathbf{R}^{n}$ be a random variable with log-concave differentiable probability density function $p: \\mathbf{R}^{n} \\rightarrow \\mathbf{R}_{+}$. We can express $p$ as $p(x)=\\exp (-\\psi(x))$, where $\\psi: \\mathbf{R}^{n} \\rightarrow \\mathbf{R}$ is convex and differentiable. <br>\n",
    "We start with the first order condition for convex functions and apply it to $\\psi$ at the point $a$ to get <br> <br>\n",
    "$\\psi(x) \\geq \\psi(a)+\\nabla \\psi(a)^{T}(x-a)$ or <br>\n",
    "$-\\psi(x) \\leq -\\psi(a)-\\nabla \\psi(a)^{T}(x-a)$ <br> <br>\n",
    "Take exponents on both sides (which does not change the sign) to get <br> <br>\n",
    "$e^{-\\psi(x)} \\leq e^{-\\psi(a)-\\nabla \\psi(a)^{T}(x-a)}$ <br>\n",
    "$e^{-\\psi(x)} \\leq e^{-\\psi(a)} e^{-\\nabla \\psi(a)^{T}(x-a)}$ <br> <br>\n",
    "Using $p(x)= e^{-\\psi(x)}$, we get <br> <br>\n",
    "$p(x) \\leq p(a) e^{-\\nabla \\psi(a)^{T}(x-a)}$ <br> <br>\n",
    "We know that $\\operatorname{prob}(X \\geq a) = \\int_{x \\geq a} p(x) dx$ so <br><br>\n",
    "$\\operatorname{prob}(X \\geq a) \\leq \\int_{x \\geq a} p(a) e^{-\\nabla \\psi(a)^{T}(x-a)} dx$ <br>\n",
    "$\\operatorname{prob}(X \\geq a) \\leq p(a)\\int_{x \\geq a} e^{-\\nabla \\psi(a)^{T}(x-a)} dx$ <br>\n",
    "$\\operatorname{prob}(X \\geq a) \\leq p(a)\\int_{x \\geq a} e^{-\\nabla \\psi(a)_{1}(x_1-a_1)}e^{-\\nabla \\psi(a)_{2}(x_2-a_2)} \\ldots e^{-\\nabla \\psi(a)_{n}(x_n-a_n)} dx$ <br>\n",
    "$\\operatorname{prob}(X \\geq a) \\leq p(a) \\Pi_{i=1}^{n} \\int_{x_{i} \\geq a_{i}} e^{-\\nabla \\psi(a)_{i}(x_i-a_i)} d x_{i}$ (using the hint) <br>\n",
    "$\\operatorname{prob}(X \\geq a) \\leq p(a) \\Pi_{i=1}^{n} \\int_{a_{i}}^{\\infty} e^{-\\nabla \\psi(a)_{i}(x_i-a_i)} d x_{i}$ <br>\n",
    "$\\operatorname{prob}(X \\geq a) \\leq p(a) \\Pi_{i=1}^{n} e^{\\nabla \\psi(a)_{i}a_i} \\int_{a_{i}}^{\\infty} e^{-\\nabla \\psi(a)_{i}x_i} d x_{i}$ <br>\n",
    "$\\operatorname{prob}(X \\geq a) \\leq p(a) \\Pi_{i=1}^{n} e^{\\nabla \\psi(a)_{i}a_i}  (0 + e^{-\\nabla \\psi(a)_{i}x_i}(\\nabla \\psi(a)_{i})^{-1}) $ <br>\n",
    "$\\operatorname{prob}(X \\geq a) \\leq p(a) \\Pi_{i=1}^{n} \\nabla \\psi(a)^{-1}_{i} $ <br>\n",
    "$\\operatorname{prob}(X \\geq a) \\leq p(a) (\\Pi_{i=1}^{n} \\nabla \\psi(a)_{i})^{-1} $ <br><br>\n",
    "Hence, we get the required upper bound. (I'm sorry you had to read that but this is the best I could do with markdown on Jupyter.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b1f64d8",
   "metadata": {},
   "source": [
    "### Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fd8d17b",
   "metadata": {},
   "source": [
    "In the specific case, $p$ is a multivariate normal distribution. Hence $\\psi$ reduces to <br>\n",
    "$\\psi (x) = \\frac{1}{2}(x^T \\Sigma x + log(2\\pi |\\Sigma|))$ and <br>\n",
    "$\\nabla \\psi (x) = \\Sigma x$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "84691bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# problem data\n",
    "\n",
    "n = 2\n",
    "rho = 0.5\n",
    "a = np.array([3, 3])\n",
    "Sigma = np.array([[1, rho], [rho, 1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "66b6ef09",
   "metadata": {},
   "outputs": [],
   "source": [
    "var = multivariate_normal(mean=[0,0], cov=Sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "854d8192",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The upper bound is 0.00011388397496548558\n"
     ]
    }
   ],
   "source": [
    "up_b = var.pdf(a)/(np.prod(np.linalg.inv(Sigma)@a))\n",
    "print(\"The upper bound is {}\".format(up_b))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce073295",
   "metadata": {},
   "source": [
    "This is clearly a much more coservative upper bound as compared to the value obtained by Monte Carlo simulations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cfe0ead",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:cvxpy_env]",
   "language": "python",
   "name": "conda-env-cvxpy_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
